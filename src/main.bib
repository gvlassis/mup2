% TP0
@article{tp0,
    title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:1902.04760},
    year={2019}
}

% TP1
@article{tp1,
    title={Wide feedforward or recurrent neural networks of any architecture are gaussian processes},
    author={Yang, Greg},
    journal={Advances in Neural Information Processing Systems},
    volume={32},
    year={2019}
}

% TP2
@article{tp2,
    title={Tensor programs ii: Neural tangent kernel for any architecture},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:2006.14548},
    year={2020}
}

% TP2b
@inproceedings{tp2b,
    title={Tensor programs iib: Architectural universality of neural tangent kernel training dynamics},
    author={Yang, Greg and Littwin, Etai},
    booktitle={International Conference on Machine Learning},
    pages={11762--11772},
    year={2021},
    organization={PMLR}
}

% TP3
@article{tp3,
    title={Tensor programs iii: Neural matrix laws},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:2009.10685},
    year={2020}
}

% TP4
@article{tp4,
    title={Feature learning in infinite-width neural networks},
    author={Yang, Greg and Hu, Edward J},
    journal={arXiv preprint arXiv:2011.14522},
    year={2020}
}

% TP4b
@inproceedings{tp4b,
    title={Adaptive optimization in the $\infty$-width limit},
    author={Littwin, Etai and Yang, Greg},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2022}
}

% TP4c
@article{tp4c,
    title={A spectral condition for feature learning},
    author={Yang, Greg and Simon, James B and Bernstein, Jeremy},
    journal={arXiv preprint arXiv:2310.17813},
    year={2023}
}

% TP5
@article{tp5,
    title={Tuning large neural networks via zero-shot hyperparameter transfer},
    author={Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    pages={17084--17097},
    year={2021}
}

% TP6
@inproceedings{tp6,
    title={Feature Learning in Infinite Depth Neural Networks},
    author={Yang, Greg and Yu, Dingli and Zhu, Chen and Hayou, Soufiane},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2023}
}

@article{alexnet,
    title={Imagenet classification with deep convolutional neural networks},
    author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    journal={Advances in neural information processing systems},
    volume={25},
    year={2012}
}

@article{gpt1,
    title={Improving language understanding by generative pre-training},
    author={Radford, Alec},
    year={2018}
}

@article{gpt2,
    title={Language models are unsupervised multitask learners},
    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
    journal={OpenAI blog},
    volume={1},
    number={8},
    pages={9},
    year={2019}
}

@article{gpt3,
    title={Language models are few-shot learners},
    author={Brown, Tom B},
    journal={arXiv preprint arXiv:2005.14165},
    year={2020}
}

@article{gpt4,
    title={Gpt-4 technical report},
    author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
    journal={arXiv preprint arXiv:2303.08774},
    year={2023}
}

@article{bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@misc{noci,
    title={Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning}, 
    author={Lorenzo Noci and Alexandru Meterez and Thomas Hofmann and Antonio Orvieto},
    year={2024},
    eprint={2402.17457},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2402.17457}, 
}

@article{cerebras,
    title={Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster},
    author={Dey, Nolan and Gosal, Gurpreet and Khachane, Hemant and Marshall, William and Pathria, Ribhu and Tom, Marvin and Hestness, Joel and others},
    journal={arXiv preprint arXiv:2304.03208},
    year={2023}
}

@article{flm,
    title={Flm-101b: An open llm and how to train it with \$100 k budget},
    author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and others},
    journal={arXiv preprint arXiv:2309.03852},
    year={2023}
}

@article{btlm,
    title={Btlm-3b-8k: 7b parameter performance in a 3b parameter model},
    author={Dey, Nolan and Soboleva, Daria and Al-Khateeb, Faisal and Yang, Bowen and Pathria, Ribhu and Khachane, Hemant and Muhammad, Shaheer and Myers, Robert and Steeves, Jacob Robert and Vassilieva, Natalia and others},
    journal={arXiv preprint arXiv:2309.11568},
    year={2023}
}

@article{cc,
    title={Llm360: Towards fully transparent open-source llms},
    author={Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and others},
    journal={arXiv preprint arXiv:2312.06550},
    year={2023}
}

@article{minicpm,
    title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
    author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
    journal={arXiv preprint arXiv:2404.06395},
    year={2024}
}

@article{teleflm,
    title={Tele-FLM Technical Report},
    author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Wang, Chao and Liu, Xinzhang and Wang, Zihan and Zhao, Yu and Wang, Xin and Huang, Yuyao and others},
    journal={arXiv preprint arXiv:2404.16645},
    year={2024}
}

@article{graphium,
    title={Towards foundational models for molecular learning on large-scale multi-task datasets},
    author={Beaini, Dominique and Huang, Shenyang and Cunha, Joao Alex and Moisescu-Pareja, Gabriela and Dymov, Oleksandr and Maddrell-Mander, Samuel and McLean, Callum and Wenkel, Frederik and M{\"u}ller, Luis and Mohamud, Jama Hussein and others},
    journal={arXiv preprint arXiv:2310.04292},
    year={2023}
}

@misc{owt,  
    title={OpenWebText Corpus},
    author={Aaron Gokaslan and Vanya Cohen},
    howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
    year={2019}
}
